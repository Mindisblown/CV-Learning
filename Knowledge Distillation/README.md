#  2014

## Distilling the Knowledge in a Neural Network

​		Hinton知识蒸馏开篇之作。思想现在来看其实比较简单，一个大的T模型学习到的东西要远超过一个小的T模型，那么我用一个loss来约束两个模型的输出，使得S模型输出的分布尽可能接近T模型，这就意味着我能学习到T模型的知识。

​		进一步在S模型预测时我要给他两个知识，一个是S模型的知识，一个是本身GT相当于课本的知识。温度的概念是为了控制经过softmax之后一些概率无限接近于0，为了使得整个类别的概率曲线变的平滑。温度越大最终拉的曲线越平。(有点类似标签平滑，让网络不要只关注正样本的概率，还要考虑负样本的概率，既要识别我有多像正样本还要识别我有多不像负样本)

